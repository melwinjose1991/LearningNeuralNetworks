{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return(sigmoid(x)*(1-sigmoid(x)))\n",
    "\n",
    "def getActivationFunctionPrime(activation_fn):\n",
    "    if activation_fn == sigmoid:\n",
    "        return sigmoidPrime\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y, reg=False, thetas=None):\n",
    "    '''\n",
    "        Args:\n",
    "            y_hat(np.ndarray) : predictions\n",
    "            y(np.ndarray) : acutal values\n",
    "            reg(python.Boolean) : to regularize or not\n",
    "            thetas(np.ndarray) : thetas to be regularized\n",
    "            \n",
    "        Returns:\n",
    "            J(float) : Cost\n",
    "    '''\n",
    "    m = y_hat.shape[0]\n",
    "    \n",
    "    J = - 1 * (1/m) * np.sum( ( np.log(y_hat)*(y) + np.log(1-y_hat)*(1-y) ) )\n",
    "\n",
    "    if reg:\n",
    "        J = J + (1/(2*m)) * ( np.sum(np.square(thetas)) )\n",
    "    \n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, weights=None, activation_fn=None, add_bias_node=False):\n",
    "        '''\n",
    "            Args:\n",
    "                activation_fn(python.function) : activation function.\n",
    "                weights(np.ndarray) : weigths\n",
    "                add_bias_node(python.Boolean) : if bias node needs to be added. If True, a column of 1s are added\n",
    "                                                after the activation function is applied.\n",
    "        '''\n",
    "        self.weights = None\n",
    "        self.activation_fn = activation_fn\n",
    "        self.activation_fn_prime = getActivationFunctionPrime(self.activation_fn) \n",
    "        self.add_bias_node = add_bias_node\n",
    "        self.input = None\n",
    "        self.z = None\n",
    "        self.output = None\n",
    "        \n",
    "        if type(weights) is np.ndarray:\n",
    "            self.weights = weights\n",
    "        \n",
    "    \n",
    "    def forward(self, input):       \n",
    "        '''\n",
    "            Args:\n",
    "                input(np.array) : input or activations from previous layer.\n",
    "                \n",
    "            Returns:\n",
    "                a(np.array) : activation of this layer. [num samples x num of nodes in this layer]\n",
    "                              If add_bias_node is True, an extra column of 1s are added.\n",
    "        '''\n",
    "        self.input = input\n",
    "        print(\"\\tInput : \", input.shape)\n",
    "        \n",
    "        if self.activation_fn != None:\n",
    "            print(\"\\tweights :\", input.shape,\" X input.T:\", self.weights.T.shape)\n",
    "            z = input.dot(self.weights.T)\n",
    "            self.z = z\n",
    "            print(\"\\tZ : \", z.shape)\n",
    "            \n",
    "            a = self.activation_fn(z)\n",
    "            \n",
    "        else:\n",
    "            a = input\n",
    "            \n",
    "        if self.add_bias_node:\n",
    "            print(\"\\tAdding a column vector (\",a.shape[0],\"x1 ) to a\", a.shape)\n",
    "            a = np.c_[np.ones((a.shape[0],1)), a]\n",
    "        \n",
    "        print(\"\\tOuput : \", a.shape)\n",
    "        self.output = a\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def backward(self, back_prop, last_layer=False, weights_next=None):\n",
    "        '''\n",
    "            Args:\n",
    "                back_prop(np.array) : info back propagated from the next layer.\n",
    "                                      Can be acutal output or deltas.\n",
    "                \n",
    "            Returns:\n",
    "                delta(np.ndarray) : delta to be backpropagated.\n",
    "                \n",
    "        '''\n",
    "        if last_layer:\n",
    "            print(\"y_hat\", self.output.shape, \" - back_prop\", back_prop.shape)\n",
    "            d = (self.output - back_prop) \n",
    "        else:\n",
    "            print(\"Weights_Next\", weights_next[:,1:].shape, \"x back_prop\", back_prop.shape, \" .* prime\", self.z.shape)\n",
    "            d = back_prop.dot(weights_next[:,1:]) * self.activation_fn_prime(self.z)\n",
    "        \n",
    "        gradient = d.T.dot(self.input)\n",
    "            \n",
    "        return d, gradient, self.weights\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, cost_fn):\n",
    "        self.layers = []\n",
    "        self.cost_fn = cost_fn\n",
    "    \n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        \n",
    "    def train(self, input, output):\n",
    "        # forward propagation\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            print(\"\\nLayer \", idx+1 )\n",
    "            input = layer.forward(input)\n",
    "        \n",
    "        y_hat = input\n",
    "        print(\"\\nPredictions\\n\", y_hat[0,:])\n",
    "        \n",
    "        # calculate cost\n",
    "        cost = self.cost_fn(y_hat, output, reg=0)\n",
    "        print(\"\\nCost : \", cost)\n",
    "        \n",
    "        # TODO: backward propagation\n",
    "        d = output\n",
    "        weights = None\n",
    "        idx = len(self.layers)-1\n",
    "        while idx >= 1:\n",
    "            print(\"\\nLayer \", idx+1)\n",
    "            d, gradient, weights = self.layers[idx].backward(d, weights_next=weights, last_layer=(idx==len(self.layers)-1) )\n",
    "            print(gradient/input.shape[0])\n",
    "            idx = idx - 1\n",
    "        \n",
    "        # return y_hat, cost\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape =  (5000, 10)\n",
      "X.shape :  (5000, 400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USMEJOS\\Downloads\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "\n",
    "data = loadmat('data/ex4data1.mat')\n",
    "\n",
    "y = data['y']\n",
    "y = pd.get_dummies(y.ravel()).as_matrix() \n",
    "print('y.shape = ',y.shape)\n",
    "\n",
    "X = data['X']\n",
    "print('X.shape : ', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta1 : (25, 401)\n",
      "theta2 : (10, 26)\n",
      "params : (10285,)\n"
     ]
    }
   ],
   "source": [
    "weights = loadmat('data/ex4weights.mat')\n",
    "theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "print('theta1 :', theta1.shape)                             # Input size : 401 including bias\n",
    "                                                            # Num of hidden units : 10\n",
    "print('theta2 :', theta2.shape)                             # Num of lables : 10\n",
    "params = np.r_[theta1.ravel(), theta2.ravel()]\n",
    "print('params :', params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer  1\n",
      "\tInput :  (5000, 400)\n",
      "\tAdding a column vector ( 5000 x1 ) to a (5000, 400)\n",
      "\tOuput :  (5000, 401)\n",
      "\n",
      "Layer  2\n",
      "\tInput :  (5000, 401)\n",
      "\tweights : (5000, 401)  X input.T: (401, 25)\n",
      "\tZ :  (5000, 25)\n",
      "\tAdding a column vector ( 5000 x1 ) to a (5000, 25)\n",
      "\tOuput :  (5000, 26)\n",
      "\n",
      "Layer  3\n",
      "\tInput :  (5000, 26)\n",
      "\tweights : (5000, 26)  X input.T: (26, 10)\n",
      "\tZ :  (5000, 10)\n",
      "\tOuput :  (5000, 10)\n",
      "\n",
      "Predictions\n",
      " [1.12661530e-04 1.74127856e-03 2.52696959e-03 1.84032321e-05\n",
      " 9.36263860e-03 3.99270267e-03 5.51517524e-03 4.01468105e-04\n",
      " 6.48072305e-03 9.95734012e-01]\n",
      "\n",
      "Cost :  0.2876291651613189\n",
      "\n",
      "Layer  3\n",
      "y_hat (5000, 10)  - back_prop (5000, 10)\n",
      "[[ 6.28737643e-04  7.50946274e-04  9.87964596e-05  1.48819864e-03\n",
      "   7.31802078e-04  1.38113760e-03 -1.59325422e-04 -6.68870887e-04\n",
      "  -1.24979363e-03 -9.66225987e-05  7.19244384e-04 -5.10976177e-04\n",
      "   1.11120644e-03 -6.43551911e-04 -6.95182470e-04 -9.47091610e-04\n",
      "   2.00794722e-04  9.50724940e-04 -5.42000276e-04 -5.05540551e-05\n",
      "   2.22327563e-04  5.06964221e-04  2.45312090e-04  1.39987507e-03\n",
      "   5.07147165e-04  8.73666979e-04]\n",
      " [ 3.64629330e-04 -3.30198116e-04  7.35246432e-04 -1.10891236e-03\n",
      "   5.08782634e-04 -5.19734068e-05  1.74945559e-04 -7.39064994e-04\n",
      "   9.64388691e-04  5.79125641e-04  1.29281909e-03  1.75949394e-03\n",
      "   1.30221701e-03 -9.87783575e-04 -2.39887091e-04 -1.07227601e-03\n",
      "   1.52381719e-03  4.91374051e-04  6.97764085e-05  1.21105864e-03\n",
      "   7.54198536e-04 -6.21040577e-04 -1.76663535e-04 -1.39495592e-03\n",
      "   1.44730496e-03 -9.03758987e-04]\n",
      " [ 3.95969117e-04  1.12882466e-03 -1.25679941e-03  1.84001416e-03\n",
      "   8.15869182e-05 -9.12851837e-04 -5.68505499e-04  9.15138853e-04\n",
      "   4.64048849e-04  4.28980738e-04 -4.64979824e-04  1.51397892e-03\n",
      "  -3.32895997e-04 -3.07445967e-04 -1.49550657e-03  1.49037022e-03\n",
      "   9.94215429e-04 -6.88537268e-04  6.89197937e-04  1.12333480e-03\n",
      "   1.11253596e-03  1.94466383e-04 -1.31811591e-04  6.99115229e-04\n",
      "  -5.51270699e-04  3.21888222e-04]\n",
      " [ 4.41308846e-04 -2.49254335e-04 -3.25817442e-04  1.18603273e-04\n",
      "  -1.13145832e-03  1.44485566e-04  1.13977144e-03 -2.76071810e-04\n",
      "  -6.07058025e-04  2.00406221e-03 -9.26612074e-04 -2.01368526e-04\n",
      "   5.80461317e-04  1.58562579e-03 -5.49024607e-04  3.06042963e-04\n",
      "   4.03405275e-04  9.67974359e-04 -8.86700248e-05  1.35797213e-04\n",
      "  -1.06682994e-03 -7.54405264e-04  1.62584412e-03  6.50737796e-05\n",
      "   1.20361986e-03  8.79809312e-04]\n",
      " [ 2.52443659e-04  1.22282207e-03 -1.28856970e-03 -1.20256027e-03\n",
      "  -1.67357357e-04  1.26920834e-03  1.78409344e-04 -1.12026605e-03\n",
      "   8.98133571e-04  6.40020340e-05 -1.76857695e-04 -5.39848044e-04\n",
      "  -7.35934645e-04  1.29041348e-04  9.37325565e-04  8.67006479e-05\n",
      "   3.73720869e-04 -6.99064544e-04 -8.69607078e-04 -1.95442516e-03\n",
      "   1.30633950e-03  1.50354974e-03  1.84757589e-03  1.05434391e-03\n",
      "   4.35748719e-04 -1.55190119e-03]\n",
      " [ 5.63471532e-04 -2.32884889e-04  5.54475155e-04 -7.07707327e-05\n",
      "   2.48878481e-04 -3.26833559e-04 -2.31513100e-04  1.60280415e-03\n",
      "  -1.35952041e-03  1.27242662e-03  1.12159102e-03 -6.27548230e-04\n",
      "  -4.56304858e-04 -1.24960483e-03  6.99968719e-04  3.22403744e-04\n",
      "  -5.83872103e-04  8.74417193e-04  1.38932037e-03 -1.49304420e-04\n",
      "   2.37414844e-04  1.36928244e-03 -1.30630233e-04  7.29394600e-04\n",
      "   1.00403989e-03  5.00051329e-04]\n",
      " [ 3.59498556e-04 -6.90830814e-04  9.12420358e-04  1.21636965e-03\n",
      "   9.04321371e-04  1.46426424e-03  1.42955507e-03  8.26048276e-04\n",
      "   5.20560586e-04  4.09724765e-04 -8.70501881e-04  9.72081317e-04\n",
      "  -3.03188312e-04  4.77174142e-04 -5.26329010e-04 -1.24303344e-03\n",
      "  -2.84119878e-04 -2.25488304e-04  7.79525983e-04  3.80617213e-05\n",
      "  -8.66392990e-04  7.10746609e-04  4.60255965e-04  1.44571671e-05\n",
      "  -9.70809278e-04 -9.84907927e-04]\n",
      " [ 5.43686742e-04  4.45690558e-04  1.95268643e-03 -1.79675107e-04\n",
      "   2.22874615e-05  5.26078830e-04 -8.74793116e-04  1.37929356e-03\n",
      "   1.15031930e-03 -5.51497242e-04  9.95140627e-04 -7.29970859e-04\n",
      "  -1.23711498e-03  1.22701926e-03  1.25047264e-03  2.12639236e-04\n",
      "   1.39420851e-03  1.80320414e-03 -4.51487804e-04  1.64887913e-03\n",
      "   7.71735888e-04 -1.09614374e-03  7.32915870e-04  2.57511498e-04\n",
      "  -3.75811463e-04  1.27957448e-03]\n",
      " [ 8.49655343e-04 -6.88848156e-05 -5.61678035e-04  7.82060264e-04\n",
      "   2.59685237e-03 -1.42445183e-05  1.76594324e-03 -2.83670734e-04\n",
      "   7.23534363e-04 -1.12572225e-03  1.30840836e-04 -5.66125198e-05\n",
      "   9.68861638e-04  5.24539442e-04  1.33314112e-03  1.82862707e-03\n",
      "  -1.21907546e-04  7.75819216e-04 -2.19923805e-04 -2.26989188e-04\n",
      "   1.31976021e-03 -3.15111809e-05 -7.74878054e-04  5.00631408e-04\n",
      "   1.13453370e-03  1.35707295e-03]\n",
      " [ 2.47554498e-04  8.40622498e-04  7.49803373e-04 -4.75889811e-04\n",
      "  -2.75108890e-04  1.63093186e-04 -3.86608063e-04  4.07194373e-04\n",
      "   4.14276469e-04 -6.44816897e-05  7.60309757e-04  1.06863698e-03\n",
      "   1.63130504e-03  1.30124972e-03  1.02705960e-03  5.40580976e-04\n",
      "  -5.61650698e-04 -7.20436373e-04  1.13423425e-03 -2.88384083e-04\n",
      "  -1.06346990e-03  8.28191172e-04 -5.38570824e-04  9.66104721e-05\n",
      "  -7.57736846e-04  7.73329872e-04]]\n",
      "\n",
      "Layer  2\n",
      "Weights_Next (10, 25) x back_prop (5000, 10)  .* prime (5000, 25)\n",
      "[[ 6.18712766e-05  0.00000000e+00  0.00000000e+00 ...  9.70102410e-09\n",
      "   2.85541159e-09  0.00000000e+00]\n",
      " [ 9.38798109e-05  0.00000000e+00  0.00000000e+00 ...  3.22774693e-08\n",
      "  -1.26316319e-10  0.00000000e+00]\n",
      " [-1.92593606e-04  0.00000000e+00  0.00000000e+00 ...  7.05404498e-08\n",
      "   1.41584916e-09  0.00000000e+00]\n",
      " ...\n",
      " [ 6.60569302e-05  0.00000000e+00  0.00000000e+00 ... -1.40472252e-08\n",
      "   1.94786057e-09  0.00000000e+00]\n",
      " [ 2.90522062e-04  0.00000000e+00  0.00000000e+00 ...  5.06149568e-07\n",
      "  -5.54722494e-08  0.00000000e+00]\n",
      " [-6.33753316e-05  0.00000000e+00  0.00000000e+00 ...  5.05491848e-09\n",
      "   4.46821993e-09  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(cost_fn=cross_entropy)\n",
    "input_layer = Layer(activation_fn=None, add_bias_node=True)\n",
    "hidden_layer = Layer(activation_fn=sigmoid, weights=theta1, add_bias_node=True)\n",
    "output_layer = Layer(activation_fn=sigmoid, weights=theta2, add_bias_node=False)\n",
    "\n",
    "nn.add(input_layer)\n",
    "nn.add(hidden_layer)\n",
    "nn.add(output_layer)\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF1.8",
   "language": "python",
   "name": "tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
